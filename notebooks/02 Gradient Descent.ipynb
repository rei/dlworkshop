{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let us implement training a neural network using gradient descent in PyTorch.\n",
    "\n",
    "PyTorch utilizes its autograd mechanism [docs](https://pytorch.org/docs/stable/notes/autograd.html) to calculate the gradients for every parameter in a computaton graph automatically, given an error.\n",
    "For this we will:\n",
    "- Build a neuron using PyTorch's API\n",
    "- Calculate the output of a neuron given its current weights\n",
    "- Calculate the error with a given label\n",
    "- Let PyTorch figure out the gradients using .backward()\n",
    "- apply the gradients with $w \\leftarrow w - \\alpha * w.grad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'\n",
    "# Let us load the data from the previous example\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "target = 'Occupancy'\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df[features], df[target])\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the artificial neuron using PyTorch's API\n",
    "PyTorch abstracts neural networks using the nn.Module class. Every neural network has to subclass from it for PyTorch's mechanisms to work perfectly. In addition, every layer has to be a member of the network's class. Otherwise the weights do not appear as parameters of the network. Let us start by building a single neuron within a PyTorch module.\n",
    "\n",
    "Building this in PyTorch is straight forward using an [nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) layer. This layer simply gets the number of inputs handed to it and the number of outputs expected. As we have 5 input features, we have 5 inputs. As we try to predict on output, so the layer needs to have 1 output. Additionally, we will need to add the sigmoid as an activation using [nn.Sigmoid](https://pytorch.org/docs/stable/nn.html#torch.nn.Sigmoid)\n",
    "\n",
    "For the actual calculation, we will override the `forward` function of the module and for numerical stabillity we will need to calculate the input to the sigmoids activation (logits) separatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs):\n",
    "        super().__init__()\n",
    "        # Build the neuron using nn.Linear\n",
    "        self.neuron = nn.Linear(number_of_inputs, 1)\n",
    "        # use nn.Sigmoid as an activation function\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def logit(self, inp):\n",
    "        # Calculate the input to the activation function\n",
    "        # Hint: Calculating the output of a layer can be\n",
    "        # done by simply calling layer(inp) and the output\n",
    "        # from the linear layer (the neuron) is the input to the activation function\n",
    "        return self.neuron(inp)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # Use the output of the logits function to \n",
    "        # calculate the output of the whole network\n",
    "        return self.act(self.logit(inp))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now grap a random selection of the training data, run them through the neuron, let PyTorch calculate the gradients and change the weights accordingly:\n",
    "- We will use [nn.BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCEWithLogitsLoss) to calculate the error.\n",
    "- We run .backward() on the loss to get the gradients for all parameters.\n",
    "- We update the weights using `w = w - alpha * w.grad` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter neuron.weight\n",
      "Parameter containing:\n",
      "tensor([[-0.3932,  0.3328,  0.4396, -0.3463,  0.3162]], requires_grad=True)\n",
      "Gradient tensor([[ 0.0415,  0.1005, -0.0275, -0.0251,  0.0598]])\n",
      "Parameter neuron.bias\n",
      "Parameter containing:\n",
      "tensor([-0.1682], requires_grad=True)\n",
      "Gradient tensor([0.2655])\n"
     ]
    }
   ],
   "source": [
    "# Build the loss\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# Build the neuron:\n",
    "neuron = Neuron(5)\n",
    "alpha = 5e-2\n",
    "select = np.random.randint(0, len(x_train), 2048)\n",
    "x = torch.from_numpy(x_train[select]).float()\n",
    "y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "y_logits = neuron.logit(x)\n",
    "err = loss(y_logits, y)\n",
    "# Let PyTorch figure out the gradients using err.backward()\n",
    "err.backward()\n",
    "# Update the weights\n",
    "for name, w in neuron.named_parameters():\n",
    "    print('Parameter {}\\n{}\\nGradient {}'.format(name, w, w.grad))\n",
    "    w = w - alpha*w.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer\n",
    "As updating the weights can be done automatically as well, PyTorch implements [optimizers](https://pytorch.org/docs/stable/optim.html) that can take care of this for us. \n",
    "We will start with the simplest optimizer [Storchastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the optimizer, by passing all model \n",
    "# parameters (all parameters of the neuron) \n",
    "# to it and the learning rate alpha to it\n",
    "optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)\n",
    "# if the error rate is moving too slow, increase it, if not you decrease it\n",
    "# don't go higher than 1^-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can reset the gradients of all associated parameters using `optim.zero_grad()` and can apply the gradients to the parameters using `optim.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(optim, loss, neuron, x, y):\n",
    "    # Reset the gradients of all parameters:\n",
    "    optim.zero_grad() #you want to calculate new gradients each time as opposed to summing them every time\n",
    "    y_pred = neuron.logit(x)\n",
    "    #print(y, y_pred, y.sum())\n",
    "    err = loss(y_pred, y)\n",
    "    #err = err * (y * 3 + 1)\n",
    "    err.backward()\n",
    "    # Apply the gradients to all parameters \n",
    "    optim.step()\n",
    "    return y_pred\n",
    "\n",
    "def eval_batch(neuron, x):\n",
    "    y_pred = neuron.logit(x)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7854443192481995\n",
      "accuracy 0.7846167087554932\n",
      "accuracy 0.7858691215515137\n",
      "accuracy 0.7946508526802063\n",
      "accuracy 0.8091503977775574\n",
      "accuracy 0.813952624797821\n",
      "accuracy 0.8271874785423279\n",
      "accuracy 0.8428589105606079\n",
      "accuracy 0.857226550579071\n",
      "accuracy 0.8688158988952637\n",
      "accuracy 0.8765698075294495\n",
      "accuracy 0.8822338581085205\n",
      "accuracy 0.8880200386047363\n",
      "accuracy 0.8923119902610779\n",
      "accuracy 0.8966235518455505\n",
      "accuracy 0.9027172923088074\n",
      "accuracy 0.9067846536636353\n",
      "accuracy 0.9097338914871216\n",
      "accuracy 0.9141333103179932\n",
      "accuracy 0.9185742139816284\n",
      "Training time: 3.1296544075012207\n"
     ]
    }
   ],
   "source": [
    "# Let's run the actual optimization\n",
    "start = time.time()  \n",
    "for i in range(20):\n",
    "    acc = None\n",
    "    for i in range(200):\n",
    "        select = np.random.randint(0, len(x_train), 2048)\n",
    "        x = torch.from_numpy(x_train[select]).float()\n",
    "        y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1)\n",
    "        y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "        if acc is None:\n",
    "            acc = (y==(y_pred > .5).float()).float().mean()\n",
    "        else:\n",
    "            acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        #y_pred = y_pred.argmax(dim=-1)\n",
    "        #acc += (y==y_pred).float().mean()\n",
    "    print('accuracy {}'.format(acc/200))\n",
    "print('Training time: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move the neuron to the GPU\n",
    "PyTorch tensors and modules allow us to call .cuda() on them to move the computations to the GPU.\n",
    "This makes it really easy to perform any calculation on the GPU (which is super handy even if you do not use neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.78494384765625\n",
      "accuracy 0.78558837890625\n",
      "accuracy 0.78591064453125\n",
      "accuracy 0.785283203125\n",
      "accuracy 0.80230712890625\n",
      "accuracy 0.80955078125\n",
      "accuracy 0.81827392578125\n",
      "accuracy 0.837685546875\n",
      "accuracy 0.8555908203125\n",
      "accuracy 0.865849609375\n",
      "accuracy 0.8770947265625\n",
      "accuracy 0.88015869140625\n",
      "accuracy 0.88622802734375\n",
      "accuracy 0.89148193359375\n",
      "accuracy 0.89751220703125\n",
      "accuracy 0.90361083984375\n",
      "accuracy 0.90551025390625\n",
      "accuracy 0.90813720703125\n",
      "accuracy 0.91462646484375\n",
      "accuracy 0.91825927734375\n",
      "Training time: 4.538154363632202\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Move the neuron to the GPU by calling to .cuda()\n",
    "    # adding .cuda() speeds up the processing possibly 30X\n",
    "    neuron = Neuron(5).cuda()\n",
    "    optim = torch.optim.SGD(neuron.parameters(), lr=5e-2)\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        acc = None\n",
    "        for i in range(200):\n",
    "            select = np.random.randint(0, len(x_train), 2048)\n",
    "            # Move x and y to the GPU by calling .cuda() on them as well:\n",
    "            x = torch.from_numpy(x_train[select]).float().cuda()\n",
    "            y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1).cuda()\n",
    "            y_pred = fit_batch(optim, loss, neuron, x, y)\n",
    "            if acc is None:\n",
    "                acc = (y==(y_pred > .5).float()).float().mean()\n",
    "            else:\n",
    "                acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        print('accuracy {}'.format(acc.data.cpu().numpy()/200))\n",
    "    print('Training time: {}'.format(time.time() - start))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the GPU version slower?\n",
    "\n",
    "Well, we need to move the data to the GPU and back. This costs us time. It normally pays off, as the computations take way longer than moving the data. In our current case the computation is very simple and the amount of data very small. This nothing the GPU is well suited for, because it can not use its advantage of performing a lot of computations in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
