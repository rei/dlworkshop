{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing the relevant packages\n",
    "# matplotlib for plots\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "# pandas to read in some data\n",
    "import pandas as pd\n",
    "# numpy to build our first perceptron\n",
    "import numpy as np\n",
    "# Train test split to do validate our findings from the perceptron training\n",
    "from sklearn.model_selection import train_test_split\n",
    "# MinMaxScaler to normalise the data before inputting them to the perceptron\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# PyTorch for neural networks\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (16, 9)\n",
    "import os\n",
    "home = os.path.expanduser(\"~\")\n",
    "data = home + '/data/workshop_data/occupancy_data/datatraining.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the occupancy data so we have something to predict\n",
    "df = pd.read_csv(data)\n",
    "target = 'Occupancy'\n",
    "features = [col for col in df.columns if target not in col and 'date' not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(df[features], df[target], shuffle=False)\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network\n",
    "To extend our previously build neuron to a neural network, we will need to add a second (third, fourth) linear layer.\n",
    "\n",
    "The first layer needs to output as many layers as the second one consumes. Try 10 for the time being.\n",
    "You will need to update the logits and forward function as well to pass through all layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_of_inputs, hidden_units):\n",
    "        super().__init__()\n",
    "        # Build the network using nn.Linear\n",
    "        self.linear1 = nn.Linear(number_of_inputs, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, 1)\n",
    "        self.dropout = nn.Dropout(0.05) # ADDED LATER TO SUPPORT THE BOTTOM\n",
    "        # use nn.Sigmoid as an activation function\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def logit(self, inp):\n",
    "        # logit = everything before the final activation\n",
    "        # Calculate the logits as the input to the last activation\n",
    "        # self.dropout(inp) # ADDED LATER: Could be used on the inputs\n",
    "        outer_layer1 = self.dropout(self.act1(self.linear1(inp)))\n",
    "        return self.linear2(outer_layer1)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        # Calculate the output of the whole network utilising the logit function\n",
    "        return self.act2(self.logit(inp))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now select a random selection of the training data and calculate the gradients for the neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_batch(optim, loss, net, x, y):\n",
    "    net.train() # tell the layers during training time to use dropout (when applied)\n",
    "    optim.zero_grad()\n",
    "    y_pred = net.logit(x)\n",
    "    err = loss(y_pred, y)\n",
    "    err.mean().backward()\n",
    "    optim.step()\n",
    "    return y_pred\n",
    "\n",
    "def eval_batch(net, x):\n",
    "    net.eval()\n",
    "    y_pred = net(x)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.8136474609375\n",
      "train accuracy 0.87852294921875\n",
      "train accuracy 0.95832763671875\n",
      "train accuracy 0.9708203125\n",
      "train accuracy 0.975986328125\n",
      "train accuracy 0.978974609375\n",
      "train accuracy 0.98043701171875\n",
      "train accuracy 0.98189208984375\n",
      "train accuracy 0.9811083984375\n",
      "train accuracy 0.98144287109375\n",
      "train accuracy 0.98181884765625\n",
      "train accuracy 0.98208251953125\n",
      "train accuracy 0.98263671875\n",
      "train accuracy 0.98270751953125\n",
      "train accuracy 0.98297607421875\n",
      "train accuracy 0.98324462890625\n",
      "train accuracy 0.9833056640625\n",
      "train accuracy 0.9833935546875\n",
      "train accuracy 0.98309814453125\n",
      "train accuracy 0.98332763671875\n",
      "val accuracy 0.924361526966095\n",
      "Training time: 7.8452582359313965\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net = Network(5, 10).cuda()\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=1e-2)\n",
    "    start = time.time()\n",
    "    for i in range(20):\n",
    "        acc = None\n",
    "        for i in range(200):\n",
    "            select = np.random.randint(0, len(x_train), 2048)\n",
    "            x = torch.from_numpy(x_train[select]).float().cuda()\n",
    "            y = torch.from_numpy(y_train.iloc[select].values).float().unsqueeze(1).cuda()\n",
    "            y_pred = fit_batch(optim, loss, net, x, y)\n",
    "            if acc is None:\n",
    "                acc = (y==(y_pred > .5).float()).float().mean()\n",
    "            else:\n",
    "                acc += (y==(y_pred > .5).float()).float().mean()\n",
    "        print('train accuracy {}'.format(acc.data.cpu().numpy()/200))\n",
    "    \n",
    "    x = torch.from_numpy(x_val).float().cuda()\n",
    "    y = torch.from_numpy(y_val.values).float().unsqueeze(1).cuda()\n",
    "    y_pred = eval_batch(net, x)\n",
    "    acc = (y==(y_pred > .5).float()).float().mean()\n",
    "    print('val accuracy {}'.format(acc.data.cpu().numpy()))\n",
    "    \n",
    "    print('Training time: {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the result change with a changing network?\n",
    "\n",
    "Now try using a bigger layersize and try adding dropout.\n",
    "How can we change the training and validation loss?\n",
    "\n",
    "- What happens if we add Dropout? [docs](https://pytorch.org/docs/stable/nn.html#dropout)\n",
    "- What happens if you add momentum or weight decay to SGD? [docs](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)\n",
    "- What happens if you use an Adam optimizer instead of SGD? [docs](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)\n",
    "- What happens if we use other activation functions? [docs](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "Hint You can add dropout using:\n",
    "```\n",
    "    self.dropout = nn.Dropout(how_many_percent_shall_be_dropped)\n",
    "    \n",
    "    def logits(x):\n",
    "        out_layer1 = self.dropout(self.act1(self.linear1(x)))\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
